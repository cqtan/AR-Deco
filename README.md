# AR-Decorator üôå
----
![ardeco01_title](https://user-images.githubusercontent.com/33485290/35540354-ccadab36-0555-11e8-9edf-3284cf3b02e5.png)

---
## Zusammenfassung

> Es soll eine mobile AR-Anwendung entwickelt werden, mit der es m√∂glich ist, M√∂belst√ºcke bzw. Gegenst√§nde virtuell in die reale Welt anzuzeigen und zu positionieren. Mit Hilfe eines Head-mounted-displays (HMD) f√ºr das Smartphone und einen Handtracking-Sensor sollte kontaktlose Interaktionen mit den virtuellen Objekten m√∂glich sein, w√§hrend man sich selbst frei in allen Richtungen drehen kann.

---

## Technologien & Hardware
* Leap Motion Controller ([LeapMotion](https://www.leapmotion.com/))
* Unity 2017.2.0f3 (mit [Vuforia SDK](https://www.vuforia.com/))
* Coloreality by TangoChen ([Link](https://github.com/TangoChen/Coloreality))
* Xcode 9.2
* iPhone 6
----
## Anforderungen
### 1) Reale Welt anzeigen
Es soll zun√§chst m√∂glich sein, mit dem HMD die reale Welt zu sehen, die von dem Smartphone aufgenommen wird.

### 2) Virtuelle Objekte anzeigen
Mit Hilfe von Vuforia-Marker sollen virtuelle Objekte in die reale Welt angezeigt werden. Dazu soll die App die Marker erkennen und das entsprechende virtuelle Objekt √ºber dem Marker kreieren. Sobald der Marker nicht mehr im Blickfeld der Kamera ist, verschwindet das Objekt wieder.

### 3) Handgesten Interaktionen
Ein Handtracking Sensor von LeapMotion soll ebenso auf dem HMD befestigt werden, damit Handgesten vor dem HMD erkannt werden k√∂nnen. M√∂gliche Interaktionen mit virtuellen Obkjekten:

* positionieren im 3D Raum
* rotieren
* skalieren
* entfernen aus der Szene


----
## M√∂gliches Szenario

Ein User f√ºhrt die 'AR-Deco' App auf dem Smartphone aus und befestigt diese an einem HMD. Der Leap Motion Sensor befindet sich auch an der vorderen Seite des HMDs. Die Kamera des Smartphones nimmt die reale Welt auf und auf dem Display wird das Aufgenommene f√ºr beide Augen jeweils angezeigt.

Der User nimmt ein Vuforia-Marker vor der Kamera, um ein virtuelles Objekt auszuw√§hlen und auf die reale Welt anzuzeigen. Damit dieses Objekt in der Szene bleibt, sollte der User dieses mit der Hand greifen (z.B.  mit einem Grab-Gesture).

Andere Interaktionen mit den virtuellen Objekten sollten auch m√∂glch sein, z.B. rotieren, skalieren und entfernen.

----
## Setup Guide
Um die Anwendung benutzen zu k√∂nnen, m√ºssen folgende Schritte durchgef√ºhrt werden:

### 1) Folgendes bereithalten:

* Computer mit Windows (f√ºr den Coloreality Server)
* iPhone (auf dem die Anwendung l√§uft)
* Leap Motion Controller
* HMD (Smartphone R√ºckenkamera muss freigehalten sein)
* Mac (f√ºr die Installation auf dem iPhone)
* Ausgedruckte VuMarker

### 2) Anwendung Installieren:
* siehe *Build and Installation (iOS)*

### 3) Ger√§te kalibrieren und Server starten:
* (Windows): Leap Motion mit dem Leap Motion Control Panel kalibrieren. Setup auf Leap Motion Seite folgen ([hier](https://developer.leapmotion.com/unity/#116)).
* (Windows): Coloreality.io Server starten auf einem. IP und Port merken. Die ausf√ºhrbare Datei befindet sich im Projekt under 'others/Coloreality/executables/ColorealityServer_PC'
* (iOS): Anwendung starten und mit dem Server verbinden.
* Smartphone und Leap am HMD befestigen.

![LeapServerSettingsHMD](https://user-images.githubusercontent.com/33485290/33844441-9f7dec92-dea0-11e7-8b59-69678a0dfd94.png)


---

## User Guide:
Die folgenden Interaktionsm√∂glicheiten sind implementiert:

* M√∂bel anzeigen mit VuMarker
* M√∂bel anvisieren
* Position der M√∂bel mit √§ndern (Grab Gesture mit 1 Hand)
* Distanz variieren (Grab und Pinch Gesture)
* Rotation (Grab Gesture mit beiden H√§nden)
* Skalieren (Pinch Gesture mit beiden H√§nden)
* L√∂schen mit VuButton

### 1) Begriffserkl√§rung:

* Grab Gesture: Wenn mit der Hand eine Faust geformt wird. Entweder mit der linken oder rechten Hand.
* Pinch Gesture: Wenn der Zeigerfinger den Daumen ber√ºhrt. Jeweils mit der linken oder rechten Hand.
* Anvisieren: In diesem Kontext leuchten die Konturen von M√∂bel gelb, wenn sie im Blickfeld zentriert sind.
* VuMarker: Ein von Vuforia generiertes Bild um virtuelle Objekte zu ankern. Wird hier benutzt, um virtuelle Objekte anzuzeigen, sobald der VuMarker von der Kamera erfasst wird.
* VuButton: Die runde Fl√§che unterhalb des VuMarkers. Simuliert einen Knopf, der bei dessen Verdeckung aktiviert wird.
* Objekt: M√∂bel und Objekte sind hier gleichzusetzen.


### 2) Interaktionen:
##### M√∂bel anzeigen:
Halte einen VuMarker vor der Kamera, damit er erfasst werden kann. Bei guten Lichverh√§ltnissen wird das Tracken erleichtert. Hierf√ºr sind 4 Exemplare im Projekt beigelegt (Sofa, Cabinett, Bed, Bin)

![ardeco02_show](https://user-images.githubusercontent.com/33485290/35540434-22e68b26-0556-11e8-825c-16f1fe5f6ece.png)

---

##### M√∂bel instanziieren und positionieren:
Beim Anvisieren eines M√∂belst√ºcks vom VuMarker kann man mit einem Grab Gesture das gleiche Objekt instanziieren und die Position √§ndern.

Somit ist das Objekt unabh√§ngig von dem VuMarker. W√§hrend das Grab Gesture erkannt wird, bleibt das Objekt im Blickfeld. So k√∂nnen auch Objekte, die weiter weg sind vom User genommen und platziert werden. Dabei muss die Hand nicht zwangsl√§ufig das Objekt ber√ºhren, sondern sie muss lediglich von der Leap Motion erkannt werden.

![ardeco03_position](https://user-images.githubusercontent.com/33485290/35540450-34c56218-0556-11e8-9929-955fa65b7c03.png)

---

##### Distanz √§ndern:
Nachdem man ein Objekt mit dem Grab Gesture erfasst hat, kann man mit der anderen Hand ein Pinch Gesture vorf√ºhren, um den Abstand zwischen Objekt und User zu √§ndern.

Sobald beide Gestures von der Leap erkannt werden, erh√∂ht sich der Abstand in Relation zum Abstand beider H√§nde. Im Umkehrschluss verringert sich der Abstand, je n√§her die H√§nde zusammenkommen.

![ardeco04_distance](https://user-images.githubusercontent.com/33485290/35540453-383d0892-0556-11e8-946e-207c9d8fd090.png)

---

##### M√∂bel rotieren:
Hat man ein Objekt anvisiert, kann man mit einem beidh√§ndigen Grab Gesture die Rotation des Objekts beeinflu√üen.

Man kann es sich so vorstellen, als h√§tte man zwischen den H√§nden einen Ball. Rotiert man den Ball im Uhrzeigersinn, dann rotiert das Objekt auch in der selben Richtung. Hierbei spielen die Position beider H√§nde eine gro√üe Rolle. Je nachdem wie diese beiden zueinander im 3D-Raum stehen, so √§ndert sich auch die Rotation des anvisierten Objekts.

![ardeco05_rotate](https://user-images.githubusercontent.com/33485290/35540457-3b71fc8e-0556-11e8-9ce5-9c6159477d43.png)

---

##### M√∂bel skalieren:
Sobald ein Objekt anvisiert wird, kann man mit einem beidh√§ndigen Pinch Gesture die Gr√∂√üe des Objekts √§ndern.

Dieses Feature funktioniert √§hnlich wie bei der obigen Erkl√§rung zur √Ñnderung der Distanz. Je gr√∂√üer der Abstand beider H√§nde sind, desto gr√∂√üer wird das Objekt.

![ardeco06_scale](https://user-images.githubusercontent.com/33485290/35540459-408c5ce6-0556-11e8-92f3-6e4219467832.png)

---

##### M√∂bel l√∂schen
Wie beim ersten Punkt, zeigt man hierf√ºr den VuMarker der f√ºr das L√∂schen von Objekten zust√§ndig ist. In diesem Fall der, der das 'Bin' Objekt (M√ºlleimer) anzeigt. Wenn dieser sichtbar ist und man ein Objekt anvisiert hat, kann man den VuButton verdecken, um das anvisierte Objekt zu l√∂schen.

![ardeco07_delete](https://user-images.githubusercontent.com/33485290/35540465-43f3400c-0556-11e8-9375-39cbee74b82c.png)

----
## Project Setup

* Bei einem neuen Projekt die Ar-Deco Assets mit der beigef√ºgten Datei 'ArDeco.unitypackage' laden.

* Entweder im Ordner 'ArLeapAssets' das Prefab 'ArLeap' in die Szene bzw. Hierarchy ziehen und die standardm√§√üige Main Camera entfernen.

* Oder unter 'ArLeapAssets/Scenes' die Szene 'VuLeap' mit der letzten Version (Bsp: _v200) ausw√§hlen.

* Um Vuforia zu aktivieren, die Player Settings im n√§chsten Abschnitt beachten.


----
## Build and Installation (iOS)
Auf dem '**Build Settings**' Fenster sollte man zun√§chst auf dem Platform '**iOS**' w√§hlen, um schlie√ülich √Ñnderungen auf dem '**Player Settings**' durchf√ºhren zu k√∂nnen.

![ar_navigatebuildsettings](https://user-images.githubusercontent.com/33485290/33219956-8541a3b8-d145-11e7-844b-bfbb03117ee9.png)

![ar_buildwindow](https://user-images.githubusercontent.com/33485290/33219958-8909e884-d145-11e7-8b0f-114c1bfbee45.png)

![ar_playersettings](https://user-images.githubusercontent.com/33485290/33219964-8bae6998-d145-11e7-9836-f9495c634005.png)

Unter dem Reiter '**Other Settings**' muss eine ID im Textfeld '**Bundle Identifier**' und im Textfeld '**Camera Usage Description**' irgendein Text eingegeben werden (Letzteres, weil sonst schwarzes Display).

![ar_othersettings](https://user-images.githubusercontent.com/33485290/33219967-8ebb570e-d145-11e7-918d-7803e1ec0c8d.png)

Unter dem Reiter '**XR Settings**' braucht '**Virtual Reality Supported**' ein H√§kchen und beide SDKs '**Vuforia**' und '**Cardboard**' m√ºssen hinzugef√ºgt werden. Cardboard ist notwendig, da sonst das Builden mit Xcode nicht funktioniert wird (Fehlende Dateien etc.).

![ar_xrsettings](https://user-images.githubusercontent.com/33485290/33219969-91694b5a-d145-11e7-8d5e-7b76ff6e471b.png)

Schlie√ülich den '**Build**' Knopf auf dem Build Settings Fenster w√§hlen und ein Ziel festlegen.

## Deploy (iOS)
In dem '**Build**' Ordner, der in Unity generiert wurde, befinden sich Xcode Projekt Dateien. Statt der gewohnten '**.xcodeproj**' Datei zu √∂ffnen, sollte man stattdessen '**.xcworkspace**' √∂ffnen, da sonst die Cardboard Dateien nicht gefunden werden (hier zB. die Podfiles).

![ar_xcodefile](https://user-images.githubusercontent.com/33485290/33219974-939a9866-d145-11e7-9498-6a4b58f2add8.png)

Nach dem Ausf√ºhren der '**xcworkspace**' Datei gelangt man auf die Build Settings, indem man links auf '**Unity-iPhone**' w√§hlt.

![ar_xcodesettings](https://user-images.githubusercontent.com/33485290/33219975-960e8b52-d145-11e7-8932-539af61a88ac.png)

Hier kann man noch den endg√ºltigen Namen der App eingeben. Im '**Signing**' Bereich muss man im Team einen Provisioning Profile ausw√§hlen (einen Kostenlosen reicht auch aus, um ein Paar Tage zu testen. Danach muss man diesen Xcode Schritt nochmal machen.

Schlie√ülich ein iOS Ger√§t per USB verbinden, das Ger√§t ausw√§hlen und √ºber '**Run**' installieren lassen. Die App sollte dann automatisch ausgef√ºhrt werden.

![ar_xcoderun](https://user-images.githubusercontent.com/33485290/33219976-980d6874-d145-11e7-9af6-728fd93ab724.png)


